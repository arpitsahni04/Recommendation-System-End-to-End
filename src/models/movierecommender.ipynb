{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import surprise\n",
    "import numpy as np\n",
    "from compress_pickle import load,dump\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating1 = pd.read_csv('../data/data_v2/ratings_v2_1.csv')\n",
    "rating2 = pd.read_csv('../data/data_v2/ratings_v2_2.csv')\n",
    "ratings = pd.concat([rating1,rating2])\n",
    "ratings.head()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings.nunique(axis=0))\n",
    "sns.barplot(x = ratings.columns, y = ratings.nunique(axis=0),palette='Blues_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis\n",
    "plt.figure(1, figsize = (16,4))\n",
    "ratings['movie_id'].value_counts()[:50].plot(kind = 'bar') #take top 50 movies\n",
    "plt.figure(2, figsize = (16,4))\n",
    "ratings['user_id'].value_counts()[:50].plot(kind = 'bar') #take top 50 users\n",
    "plt.figure(3, figsize = (8,4))\n",
    "ratings['rate'].plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = ratings.groupby('user_id')['movie_id'].count() \n",
    "ratings_per_user.hist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_movie = ratings.groupby('movie_id')['user_id'].count() \n",
    "ratings_per_movie.hist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise import SVD\n",
    "\n",
    "#kNN\n",
    "similarity = {\n",
    "    \"name\":\"cosine\",\n",
    "    \"user_based\": False #item-based similarity\n",
    "}\n",
    "algo_KNN = KNNWithMeans(sim_options=similarity)\n",
    "\n",
    "#SVD\n",
    "algo_SVD = SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_small = ratings.sample(50000)\n",
    "\n",
    "# movie_rating_set = pd.crosstab(index = ratings_small.user_id, columns = ratings_small.movie_id, values = ratings_small.rate, aggfunc = np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique users in the dataset:\",ratings.user_id.unique().size)\n",
    "print(\"Number of unique movies in the dataset:\",ratings.movie_id.unique().size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "# load df into Surprise Reader object\n",
    "reader = Reader(rating_scale = (0,5))\n",
    "rating_df = Dataset.load_from_df(ratings[['user_id', 'movie_id', 'rate']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# define train test function\n",
    "def train_test_algo(algo, label):\n",
    "    training_set, testing_set = train_test_split(rating_df, test_size = 0.2)\n",
    "    start_time = time.time()\n",
    "    algo.fit(training_set)\n",
    "    end_time = time.time()\n",
    "    Training_time = end_time - start_time\n",
    "    print(\"Training Time :\",Training_time)\n",
    "    test_output = algo.test(testing_set)\n",
    "    test_df = pd.DataFrame(test_output)\n",
    "    \n",
    "    print(\"RMSE -\",label, accuracy.rmse(test_output, verbose = False))\n",
    "    print(\"MAE -\", label, accuracy.mae(test_output, verbose=False))\n",
    "    print(\"MSE -\", label, accuracy.mse(test_output, verbose=False))\n",
    "    \n",
    "    return algo,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test result\n",
    "knn,train_test_KNN = train_test_algo(algo_KNN, \"algo_KNN\")\n",
    "\n",
    "#compressing the pickle files\n",
    "dump(knn, 'KNN/algo_KNN.pkl', compression=\"lzma\", set_default_extension=False)\n",
    "\n",
    "# surprise.dump.dump('KNN/algo_KNN.pkl',algo=knn)\n",
    "print(train_test_KNN.head())\n",
    "\n",
    "svd,train_test_SVD = train_test_algo(algo_SVD, \"algo_SVD\")\n",
    "\n",
    "#compressing the pickle files\n",
    "dump(svd, 'SVD/algo_SVD.pkl', compression=\"lzma\", set_default_extension=False)\n",
    "\n",
    "# surprise.dump.dump('SVD/algo_SVD.pkl',algo=svd)\n",
    "print(train_test_SVD.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide Top Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(\"../data/data_v2/movie_info_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(algo, user_id):\n",
    "    pred_list = []\n",
    "    for movieId in range(26826):\n",
    "        rating = algo.predict(user_id, movieId).est\n",
    "        pred_list.append([user_id, movieId, rating])\n",
    "    pred_df = pd.DataFrame(pred_list, columns = ['user_id', 'movie_id', 'rate'])\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_recommendations(pred_df, top_N):\n",
    "    recommended_movie = pd.merge(pred_df, movie_df, how='left', left_on='movie_id', right_on='movie_id')[['user_id', 'movie_id', 'rate', 'title']]\n",
    "    sorted_df = recommended_movie.groupby(('user_id'), as_index = False).apply(lambda x: x.sort_values(['rate'], ascending = False)).reset_index(drop=True)\n",
    "    top_recommended_movies = sorted_df.groupby('user_id').head(top_N)\n",
    "    return sorted_df, top_recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncompress and load the pickle files\n",
    "algo_KNN = load('KNN/algo_KNN.pkl', compression=\"lzma\", set_default_extension=False)\n",
    "\n",
    "algo_SVD = load('SVD/algo_SVD.pkl', compression=\"lzma\", set_default_extension=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "algo_KNN_size = os.path.getsize('KNN/algo_KNN.pkl')\n",
    "algo_KNN_size /=(1024*1024)\n",
    "print(algo_KNN_size)\n",
    "algo_SVD_size = os.path.getsize('SVD/algo_SVD.pkl')\n",
    "algo_SVD_size /=(1024*1024)\n",
    "print(algo_SVD_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN predictions\n",
    "import time\n",
    "total_inference_time_knn = 0\n",
    "total_inference_time_svd = 0\n",
    "random_user_ids = list(ratings['user_id'].sample(n=10, replace=False))\n",
    "for query in random_user_ids:\n",
    "    start_time = time.time()\n",
    "    pred_KNN = prediction(algo_KNN, query)\n",
    "    recommended_movies_KNN, top_recommended_movies_KNN = top_recommendations(pred_KNN, 20)\n",
    "    end_time = time.time()\n",
    "    inference_time_knn = end_time - start_time\n",
    "    total_inference_time_knn +=inference_time_knn\n",
    "    \n",
    "for query in random_user_ids:\n",
    "    start_time = time.time()\n",
    "    pred_SVD = prediction(algo_SVD, query)\n",
    "    recommended_movies_SVD, top_recommended_movies_SVD = top_recommendations(pred_SVD, 20)\n",
    "    end_time = time.time()\n",
    "    inference_time_svd = end_time - start_time\n",
    "    total_inference_time_svd +=inference_time_svd\n",
    "    \n",
    "average_inference_time_svd = total_inference_time_svd/len(random_user_ids)\n",
    "average_inference_time_knn =total_inference_time_knn / len(random_user_ids)\n",
    "print(\"knn time :\",average_inference_time_knn)\n",
    "print(\"SVD time :\",average_inference_time_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recommended_movies_SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recommended_movies_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a83638afb6f32a4fd7830293d7948fd0b611a1a7bcf03aa27a8d623f81ff993"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
